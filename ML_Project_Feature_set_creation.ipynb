{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a28c9b4d9c644f69aab1821ed57b65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c7eefe34c2748988b111919fd3aa921",
              "IPY_MODEL_542ed864081c48059b7e0dee52740231",
              "IPY_MODEL_3e301883799f49d481c53a4538851a19"
            ],
            "layout": "IPY_MODEL_0bbc079d835a4e54a7484062fc33ad94"
          }
        },
        "2c7eefe34c2748988b111919fd3aa921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff139b3b02274c789281dc1687d1d086",
            "placeholder": "​",
            "style": "IPY_MODEL_ccbccc5563f24f71abfdaf64756824b5",
            "value": "100%"
          }
        },
        "542ed864081c48059b7e0dee52740231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e1da346a23948228da1a649e5d4c8e9",
            "max": 369494,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c5d7e2487714bb8b456d0472dedf027",
            "value": 369494
          }
        },
        "3e301883799f49d481c53a4538851a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aa6ea9d6b06481e93588e30b0fdf600",
            "placeholder": "​",
            "style": "IPY_MODEL_98d019214bff4b4e9d9c19447ab9e7b1",
            "value": " 369494/369494 [19:35&lt;00:00, 188.78it/s]"
          }
        },
        "0bbc079d835a4e54a7484062fc33ad94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff139b3b02274c789281dc1687d1d086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccbccc5563f24f71abfdaf64756824b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e1da346a23948228da1a649e5d4c8e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5d7e2487714bb8b456d0472dedf027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3aa6ea9d6b06481e93588e30b0fdf600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98d019214bff4b4e9d9c19447ab9e7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3OAHpriL5lPO"
      },
      "outputs": [],
      "source": [
        "# Install below packages if needed\n",
        "!pip install nltk\n",
        "!pip install cmudict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from lexicalrichness import LexicalRichness\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import warnings\n",
        "from string import punctuation\n",
        "\n",
        "from nltk.tokenize.api import TokenizerI\n",
        "from nltk.util import ngrams\n",
        "\n",
        "import cmudict\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "iagyii9dVrwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa2e12b-3266-4b3c-f269-2b247d93149a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUAq8mcyjtp5",
        "outputId": "6b5b0aa8-db69-4417-ec3a-1a8566d66006"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HC2C7Rjgj-e",
        "outputId": "bc3b9960-6343-4e03-cb20-2f706d9feee4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asian = pd.read_csv(\"/DATA/Processed/asian.csv\")\n",
        "black = pd.read_csv(\"/DATA/Processed/blackPeople.csv\")\n",
        "white = pd.read_csv(\"/DATA/Processed/whitePeople.csv\")\n",
        "native = pd.read_csv(\"/DATA/Processed/nativeAmerican.csv\")"
      ],
      "metadata": {
        "id": "9raqvqJfV5vP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(black['body'][50:70])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfuC09BnbL-8",
        "outputId": "19d96e08-8e9a-4273-cfbc-8ad9e7f4e8b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50    At least two of his kids don’t have to worry a...\n",
            "51    No. But I had friends I a ton of time with and...\n",
            "52    Rule #1. This is a sub for the black community...\n",
            "53                                                 Yes.\n",
            "54    I voluntarily signed for myself out of high-sc...\n",
            "55    All of this. And hugs were never really a thin...\n",
            "56    Nope. Not as a kid\\n\\n\\nLater on when I was an...\n",
            "57    I’m starting to think that the majority of ppl...\n",
            "58    So basically the EWF an American organization ...\n",
            "59    Nick Canon is trying to promote animal husband...\n",
            "60                                   Great interview...\n",
            "61    So firstly, there is a distinction between con...\n",
            "62                             This! ALL of this! THIS!\n",
            "63    why? more Native Americans died, and we are no...\n",
            "64    Memphis is the 26th most dangerous city in the...\n",
            "65        I could absolutely do that! I’ll message you!\n",
            "66    &gt;but they were allowed to at least exist un...\n",
            "67    I figured... I do not live anywhere near Memph...\n",
            "68    I’d love something like this done of me! I cou...\n",
            "69    I really hope he just give some of his money t...\n",
            "Name: body, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data = pd.read_csv(\"/content/drive/MyDrive/DATA/Race/Processed/race.csv\")"
      ],
      "metadata": {
        "id": "Aifs536MXcat"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(data.columns)\n",
        "#print(len(data))"
      ],
      "metadata": {
        "id": "Q2tmeaP_Md8D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_total_length(df, x):\n",
        "  len = df[x].str.len().sum()\n",
        "  return len"
      ],
      "metadata": {
        "id": "DSKsguQoWBJX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean(df):\n",
        "  return df['body'].str.len().mean()\n",
        "\n",
        "def get_std(df):\n",
        "  return df['body'].str.len().std()"
      ],
      "metadata": {
        "id": "XUmFDHXxZR7x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_asian = get_total_length(asian, 'body')\n",
        "len_black = get_total_length(black, 'body')\n",
        "len_white = get_total_length(white, 'body')\n",
        "len_native = get_total_length(native, 'body')"
      ],
      "metadata": {
        "id": "O1WahS1rWxop"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len_asian)\n",
        "print(len_black)\n",
        "print(len_white)\n",
        "print(len_native)"
      ],
      "metadata": {
        "id": "CQ4NzogHXJuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7f9cff-2f99-4281-beff-d62d66e3e020"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23774794\n",
            "41295450\n",
            "15663327\n",
            "20589033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we calculate the average length of the text for each class."
      ],
      "metadata": {
        "id": "VP8AGSgSS4Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asian_mean = get_mean(asian)\n",
        "black_mean = get_mean(black)\n",
        "white_mean = get_mean(white)\n",
        "native_mean = get_mean(native)\n",
        "print(white_mean)\n",
        "print(black_mean)\n",
        "print(asian_mean)\n",
        "print(native_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcT3zmO_ZeW-",
        "outputId": "5ee8c475-3ae9-4350-d63f-f26f52ef54c2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "159.034693877551\n",
            "438.9957264957265\n",
            "254.75541125541125\n",
            "211.63842975206612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asian_std = get_std(asian)\n",
        "black_std = get_std(black)\n",
        "white_std = get_std(white)\n",
        "native_std = get_std(native)\n",
        "print(white_std)\n",
        "print(black_std)\n",
        "print(asian_std)\n",
        "print(native_std)"
      ],
      "metadata": {
        "id": "c4a8sFr0aFbF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asian['text_len_norm'] = (asian['body'].str.len()-asian_mean)/asian_std\n",
        "black['text_len_norm'] = (black['body'].str.len()-black_mean)/black_std\n",
        "white['text_len_norm'] = (white['body'].str.len()-white_mean)/white_std\n",
        "native['text_len_norm'] = (native['body'].str.len()-native_mean)/native_std"
      ],
      "metadata": {
        "id": "mNi3rB_iXZB0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(black['body'][3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30czkKDdX45I",
        "outputId": "bb9ca11a-0d1e-48b2-80ac-261e749c6902"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You’re doing what’s best for you and that’s a good thing.  If you feel you don’t need it at this time, then yes, cut it off.  I think your mom is trying to “prepare you for the future” (lol) in her own way but need to understand that you’re an adult and you have to do what you need to take care of yourself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = asian.append(black)\n",
        "temp1 = temp.append(white)\n",
        "data = temp1.append(native)\n",
        "\n",
        "data.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "zWhwLKXfXc06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817951f1-94be-4d49-bcbb-af21d363951c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-d2e06a854ba8>:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  temp = asian.append(black)\n",
            "<ipython-input-21-d2e06a854ba8>:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  temp1 = temp.append(white)\n",
            "<ipython-input-21-d2e06a854ba8>:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = temp1.append(native)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_length'] = data['body'].str.len()"
      ],
      "metadata": {
        "id": "h1CrPb0-GUua"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average word embedding vector score\n",
        "# importing all necessary modules\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        " \n",
        "warnings.filterwarnings(action = 'ignore')\n",
        " \n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "'''from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.utils import simple_preprocess'''\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ExNpAvqMXeQp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "P8iC-sk1wgbc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNVpoywFGkoZ",
        "outputId": "0614ba99-09f1-418c-958f-3154ceb1c597"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'wasn', 'on', 'not', 'were', 'we', 'she', 'if', 'because', 'that', 'some', 're', \"doesn't\", 'i', 'itself', 'between', 'herself', 'as', 'off', 'more', 'with', \"should've\", 'whom', 'couldn', 'you', 'will', 'our', 'such', 'your', 'here', 'isn', 'shouldn', 'an', 'why', \"didn't\", 'by', 'against', 'are', 'who', 'his', 'does', \"mustn't\", \"it's\", 'these', 'and', 'from', 'd', \"wasn't\", \"you're\", 'own', 'which', 'or', 'once', 'mightn', 'those', 'weren', 'her', 'theirs', 'further', 'at', 'in', 'have', \"shouldn't\", 'all', 'just', \"haven't\", \"isn't\", 'each', 'aren', 'them', 'wouldn', 'ain', 'below', 'both', 'has', 'during', \"mightn't\", \"won't\", 'myself', 'now', 'under', 'don', 'himself', 'm', \"you'd\", 'y', 'same', 'mustn', 'him', 'only', 'most', 'themselves', 'when', 'it', 'o', 'they', 'about', 'hadn', 'll', \"weren't\", 'before', 'again', 've', \"needn't\", 'hers', 'ma', 'did', 'doesn', 'having', 'other', 'very', \"you'll\", 'so', 'how', 's', 'through', 'then', 'nor', 'can', 'won', \"wouldn't\", 'the', 'to', 'was', 'he', 'until', 'while', 'for', 'yours', 'any', 'few', 'am', 'where', 'needn', 'my', \"don't\", 'into', 'ours', 'hasn', 'being', 'than', 'up', 'should', 't', \"shan't\", 'what', \"she's\", \"you've\", 'had', 'be', 'me', 'shan', 'its', 'is', 'ourselves', 'after', 'doing', \"couldn't\", 'down', 'a', \"aren't\", 'no', 'out', 'yourself', 'haven', 'over', 'do', \"that'll\", 'of', 'this', 'their', 'there', 'too', 'but', \"hasn't\", 'been', \"hadn't\", 'above', 'yourselves', 'didn'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(data):\n",
        "    \n",
        "    text = []\n",
        "    for i in tqdm(range(len(data))):\n",
        "      if data[i] !=\"\":\n",
        "        # Sentence tokenization using NLTK library\n",
        "          temp = []\n",
        "\n",
        "          for word in word_tokenize(data[i]):\n",
        "             if word not in stop_words:\n",
        "              temp.append(word.lower())\n",
        "                \n",
        "\n",
        "      text.append(temp)\n",
        "\n",
        "\n",
        "      '''for i in sent_tokenize(text):\n",
        "        temp = []\n",
        "        # tokenize the sentence into words\n",
        "        for j in word_tokenize(i):\n",
        "            if j not in STOPWORDS:\n",
        "              temp.append(j.lower())\n",
        "              #temp.append(j.lower())\n",
        "              data.append(temp)'''\n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "p4xPXdFjXpMM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = data['cleaned_body'].values"
      ],
      "metadata": {
        "id": "U35dYpYnY1-N"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words = process_text(texts)"
      ],
      "metadata": {
        "id": "j-1HFhqcioEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fb1da6-65db-4783-9e8f-4807af79da36"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 383166/383166 [01:53<00:00, 3376.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['cleaned_body'][10])\n",
        "#print(data_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C10qUR8DGB1y",
        "outputId": "5d625e2b-0428-4adf-ee7c-5fd0db4549ec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sothen just do what he suggested and remove the problematic comments and leave the rest of the thread  Im perplexed as to how thats a problem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1La0mQ3yHbbM",
        "outputId": "30c82722-a41b-465c-9f7b-fe18098a1346"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "383166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['token_text'] = data_words"
      ],
      "metadata": {
        "id": "GhcL8DAvC7IC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['token_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcap7G5oDHGy",
        "outputId": "99961fd1-7b72-493c-86ea-9010e637cff0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0         [gt, but, retirement, privilege, often, one, b...\n",
            "1         [singaporean, authorities, wont, bother, youre...\n",
            "2                                                    [whos]\n",
            "3         [ive, called, paranoid, bringing, too, many, c...\n",
            "4                     [i, live, west, we, definitely, post]\n",
            "                                ...                        \n",
            "383161    [i, think, clearly, missing, picture, magnific...\n",
            "383162    [ignorant, european, did, enter, see, owners, ...\n",
            "383163    [as, white, metal, head, hillbilly, guy, growi...\n",
            "383164    [consultation, inquiries, frequently, asked, a...\n",
            "383165    [only, $, 15k, she, deserves, much, like, leas...\n",
            "Name: token_text, Length: 383166, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(data_words, min_count = 1,vector_size=100, window = 5)"
      ],
      "metadata": {
        "id": "c2hFSUwbhJQB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count vectorization of text\n",
        "'''from sklearn.feature_extraction.text import CountVectorizer\n",
        "  \n",
        "# Creating the vectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        " \n",
        "# Converting the text to numeric data\n",
        "X = vectorizer.fit_transform(texts)\n",
        " \n",
        "#print(vectorizer.get_feature_names())\n",
        " \n",
        "# Preparing Data frame For machine learning\n",
        "# Priority column acts as a target variable and other columns as predictors\n",
        "CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
        "CountVectorizedData['Race']=data['race']\n",
        "print(CountVectorizedData.shape)\n",
        "CountVectorizedData.head()'''"
      ],
      "metadata": {
        "id": "0eedkrldZEHL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "850192ab-0450-44ae-a73c-0b7fcb3e3f82"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from sklearn.feature_extraction.text import CountVectorizer\\n  \\n# Creating the vectorizer\\nvectorizer = CountVectorizer(stop_words='english')\\n \\n# Converting the text to numeric data\\nX = vectorizer.fit_transform(texts)\\n \\n#print(vectorizer.get_feature_names())\\n \\n# Preparing Data frame For machine learning\\n# Priority column acts as a target variable and other columns as predictors\\nCountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\\nCountVectorizedData['Race']=data['race']\\nprint(CountVectorizedData.shape)\\nCountVectorizedData.head()\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(word2vec.wv.vectors[0])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        #X = MyTokenizer().fit_transform(X)\n",
        "        \n",
        "        return np.array([\n",
        "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.transform(X)"
      ],
      "metadata": {
        "id": "V5vvSpfZHxTc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_embedding_vectorizer = MeanEmbeddingVectorizer(model)\n",
        "mean_embedded = mean_embedding_vectorizer.fit_transform(data['token_text'])"
      ],
      "metadata": {
        "id": "tywMvE8qkeZ9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = (np.mean(mean_embedded,axis=1))\n",
        "data['mean_w2v'] = x"
      ],
      "metadata": {
        "id": "5bynKDUzmcrm"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['mean_w2v'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kipE9-4ojiC",
        "outputId": "2759050f-0052-4dad-ebfa-9fec27db2b63"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    383166.000000\n",
            "mean          0.041678\n",
            "std           0.105415\n",
            "min          -0.919145\n",
            "25%          -0.010875\n",
            "50%           0.046103\n",
            "75%           0.099156\n",
            "max           0.529365\n",
            "Name: mean_w2v, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhXud64pozfi",
        "outputId": "dcb204be-21f7-4b3c-9861-f06e747a577b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['subreddit_id', 'author', 'body', 'cleaned_body', 'race',\n",
            "       'text_len_norm', 'text_length', 'token_text', 'mean_w2v'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data.to_csv(\"/content/drive/MyDrive/DATA/Race/Processed/race_data.csv\",index=False)\n",
        "#data.to_csv(\"/content/drive/MyDrive/DATA/Race/Processed/New/race_data.csv\",index=False)"
      ],
      "metadata": {
        "id": "IQbQ8dLUpj8R"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install lexical-diversity\n",
        "!pip install lexicalrichness"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DO5MQ_s6RcV",
        "outputId": "89e3d082-e98c-4de8-d8f8-084f18d4805d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lexicalrichness\n",
            "  Downloading lexicalrichness-0.5.0.tar.gz (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (1.10.1)\n",
            "Requirement already satisfied: textblob>=0.15.3 in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (0.17.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (3.7.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.0.0->lexicalrichness) (1.22.4)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob>=0.15.3->lexicalrichness) (3.8.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (4.39.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (23.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lexicalrichness) (2022.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lexicalrichness) (1.16.0)\n",
            "Building wheels for collected packages: lexicalrichness\n",
            "  Building wheel for lexicalrichness (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lexicalrichness: filename=lexicalrichness-0.5.0-py3-none-any.whl size=15534 sha256=c6c69d3c24aaef359850c664c2d27f2342cc6ecffb86abb5bddbecece847d292\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/68/59/1edd70c2b91dc172fa208eb34799e90bc6c093bfbb862ff017\n",
            "Successfully built lexicalrichness\n",
            "Installing collected packages: lexicalrichness\n",
            "Successfully installed lexicalrichness-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data = pd.read_csv(\"/content/drive/MyDrive/DATA/Race/Processed/race_data.csv\")\n",
        "data = pd.read_csv(\"/DATA/Processed/race_data.csv\")"
      ],
      "metadata": {
        "id": "1xUKVHt26UUS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import these modules\n",
        "\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "8A-KoIs7efQy"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''l = []\n",
        "for i in tqdm(range(0,len(data)),desc = 'Progress'):\n",
        "  #print(x)\n",
        "  lemm_text = lemmatizer.lemmatize(data['cleaned_body'][i])\n",
        "  l.append(ld.mtld(lemm_text))'''"
      ],
      "metadata": {
        "id": "FtdQ1zCvegwA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cccb3452-2f69-43db-fe5d-5e0cf3c2e1b0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"l = []\\nfor i in tqdm(range(0,len(data)),desc = 'Progress'):\\n  #print(x)\\n  lemm_text = lemmatizer.lemmatize(data['cleaned_body'][i])\\n  l.append(ld.mtld(lemm_text))\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''def lemm_process(text):\n",
        "    lemm_text = lemmatizer.lemmatize(text)\n",
        "    return lemm_text\n",
        "data['lemm_tokens'] = data['token_text'].apply(lemm_process)'''"
      ],
      "metadata": {
        "id": "EYn5HCHjqPrj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7485a3a7-b2f0-4462-b077-6229e8a0a828"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def lemm_process(text):\\n    lemm_text = lemmatizer.lemmatize(text)\\n    return lemm_text\\ndata['lemm_tokens'] = data['token_text'].apply(lemm_process)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['cleaned_body'].isnull().any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fsSbgl8rVmp",
        "outputId": "f539cb08-3bbc-4168-c452-d4420f73b076"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['token_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-7OK3uRrcuz",
        "outputId": "d1be115c-0acc-45a0-8b39-c8d5ece87485"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0         ['yeah', 'honestly', 'even', 'black', 'person'...\n",
            "1         ['the', 'opposite', 'experience', 'rarely', 'f...\n",
            "2         ['memphis', '26th', 'dangerous', 'city', 'enti...\n",
            "3         ['compare', 'message', 'wandering', 'earth', '...\n",
            "4         ['if', 'youre', 'serious', 'semiserious', 'cou...\n",
            "                                ...                        \n",
            "369489                            ['thanks', 'ill', 'tell']\n",
            "369490    ['i', 'heard', 'black', 'guy', 'canada', 'tell...\n",
            "369491    ['did', 'report', 'youtube', 'administration',...\n",
            "369492                                  ['zebra', 'people']\n",
            "369493                        ['i', 'get', 'hell', 'state']\n",
            "Name: token_text, Length: 369494, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[~data['cleaned_body'].str.isnumeric()]\n",
        "data.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "ybIi9hSJuB_3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['cleaned_body'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gND-VMYnuP1X",
        "outputId": "9d20d2e5-4983-41d0-962a-e30184dcb86d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0         Yeah honestly even as a black person from up n...\n",
            "1         The opposite experience here have rarely found...\n",
            "2         Memphis is the 26th most dangerous city in the...\n",
            "3         compare the message from wandering earth 2 and...\n",
            "4         If youre serious or semiserious you could look...\n",
            "                                ...                        \n",
            "369489                                  Thanks Ill tell her\n",
            "369490    I heard a black guy here in Canada telling me ...\n",
            "369491    Did you report it to YouTube administration Th...\n",
            "369492                                         Zebra people\n",
            "369493             I have to get the hell out of this state\n",
            "Name: cleaned_body, Length: 369494, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "def mtld(text):\n",
        "    lex = LexicalRichness(text)\n",
        "    return lex.mtld()\n",
        "\n",
        "data['ld_mtld'] = data['cleaned_body'].progress_apply(mtld)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK0Q82GflsL4",
        "outputId": "7f00aeb9-b7aa-476c-b80b-7f1e25f4ee2c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 369494/369494 [00:42<00:00, 8780.00it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['ld_mtld'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0K9dghoxGGw",
        "outputId": "8247685a-fd07-4686-8c64-3ac622b40cec"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0          95.873689\n",
            "1         134.560000\n",
            "2          73.173333\n",
            "3          11.000000\n",
            "4         116.640000\n",
            "             ...    \n",
            "369489      4.000000\n",
            "369490    224.000000\n",
            "369491     13.000000\n",
            "369492      2.000000\n",
            "369493     10.000000\n",
            "Name: ld_mtld, Length: 369494, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def maas(text):\n",
        "    try:\n",
        "      lex = LexicalRichness(text)\n",
        "      output = lex.Maas\n",
        "    except ZeroDivisionError:\n",
        "      output = \"\" \n",
        "    return output\n",
        "\n",
        "data['ld_maas'] = data['cleaned_body'].progress_apply(maas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9q-SIQzuTqz",
        "outputId": "2d2f3dfb-cda7-4829-c2fb-faa30564039a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 369494/369494 [00:11<00:00, 30832.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hdd(text):\n",
        "    lex = LexicalRichness(text)\n",
        "    return lex.hdd(draws=1)\n",
        "\n",
        "data['ld_hdd'] = data['cleaned_body'].progress_apply(hdd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUsFVEDIuh66",
        "outputId": "6720fe72-624f-4290-d2fd-50b62628b750"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 369494/369494 [45:56<00:00, 134.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.downloader.download('vader_lexicon')\n",
        "\n",
        "senti = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqvivdnpgPDj",
        "outputId": "b6618037-faf3-4852-bbcd-c204fe03ba39"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_sent = []\n",
        "for i in tqdm(range(0,len(data)),desc = 'Progress'):\n",
        "  text_sent.append(senti.polarity_scores(data['cleaned_body'][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWSqsEvfhavU",
        "outputId": "e9c4a517-0ab0-44bb-eda9-92d35d9e796d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress: 100%|██████████| 369494/369494 [04:04<00:00, 1511.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_sent[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUORvPsfjI5o",
        "outputId": "c09d4170-7d3f-4fca-d784-cbc77075468e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'neg': 0.179, 'neu': 0.713, 'pos': 0.108, 'compound': -0.9966}, {'neg': 0.153, 'neu': 0.806, 'pos': 0.04, 'compound': -0.8074}, {'neg': 0.057, 'neu': 0.902, 'pos': 0.042, 'compound': -0.2247}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.011, 'neu': 0.842, 'pos': 0.147, 'compound': 0.9393}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['sentiment_scores'] = text_sent"
      ],
      "metadata": {
        "id": "y4_8nLzegPM8"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data.to_csv(\"/content/drive/MyDrive/DATA/Race/Processed/New/race_data.csv\",index=False)"
      ],
      "metadata": {
        "id": "Fe-8pZBUMQHx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "id": "bW4vvS5_lh2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78e6e92-694b-4b4b-f1f0-45e9337ce33f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['subreddit_id', 'author', 'body', 'cleaned_body', 'race',\n",
            "       'text_len_norm', 'text_length', 'token_text', 'mean_w2v', 'ld_mtld',\n",
            "       'ld_maas', 'ld_hdd', 'syllable_count', 'percent_diff_words', 'NN', 'CC',\n",
            "       'VBZ', 'DT', 'RB', 'CD', 'WDT', 'JJ', 'NNS', 'IN', 'PRP$', 'NNP', 'VBN',\n",
            "       'TO', 'VBG', 'WP', 'WRB', 'VBD', 'NNPS', 'VBP', 'JJS', 'PRP', 'VB',\n",
            "       'JJR', 'MD', 'RP', 'RBR', 'EX', 'FW', 'RBS', 'PDT', 'UH', '$', 'WP$',\n",
            "       'LS', 'pos_sent', 'neg_sent', 'neu_sent', 'sentiment_scores'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip -r /content/drive/MyDrive/DATA/Race/Processed/New/race_data.zip /content/drive/MyDrive/DATA/Race/Processed/New/race_data.csv"
      ],
      "metadata": {
        "id": "r7rdWMledo8j"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = pd.read_csv(\"/content/drive/MyDrive/DATA/Race/Processed/New/race_data.csv\")"
      ],
      "metadata": {
        "id": "oPdyt4lA0sYw"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Natural Language Toolkit: Tokenizers\n",
        "#\n",
        "# Copyright (C) 2001-2020 NLTK Project\n",
        "# Author: Christopher Hench <chris.l.hench@gmail.com>\n",
        "#         Alex Estes\n",
        "# URL: <http://nltk.sourceforge.net>\n",
        "# For license information, see LICENSE.TXT\n",
        "\n",
        "\"\"\"\n",
        "The Sonority Sequencing Principle (SSP) is a language agnostic algorithm proposed\n",
        "by Otto Jesperson in 1904. The sonorous quality of a phoneme is judged by the\n",
        "openness of the lips. Syllable breaks occur before troughs in sonority. For more\n",
        "on the SSP see Selkirk (1984).\n",
        "\n",
        "The default implementation uses the English alphabet, but the `sonority_hiearchy`\n",
        "can be modified to IPA or any other alphabet for the use-case. The SSP is a\n",
        "universal syllabification algorithm, but that does not mean it performs equally\n",
        "across languages. Bartlett et al. (2009) is a good benchmark for English accuracy\n",
        "if utilizing IPA (pg. 311).\n",
        "\n",
        "Importantly, if a custom hiearchy is supplied and vowels span across more than\n",
        "one level, they should be given separately to the `vowels` class attribute.\n",
        "\n",
        "References:\n",
        "- Otto Jespersen. 1904. Lehrbuch der Phonetik.\n",
        "  Leipzig, Teubner. Chapter 13, Silbe, pp. 185-203.\n",
        "- Elisabeth Selkirk. 1984. On the major class features and syllable theory.\n",
        "  In Aronoff & Oehrle (eds.) Language Sound Structure: Studies in Phonology.\n",
        "  Cambridge, MIT Press. pp. 107-136.\n",
        "- Susan Bartlett, et al. 2009. On the Syllabification of Phonemes.\n",
        "  In HLT-NAACL. pp. 308-316.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class SyllableTokenizer(TokenizerI):\n",
        "    \"\"\"\n",
        "    Syllabifies words based on the Sonority Sequencing Principle (SSP).\n",
        "\n",
        "        >>> from nltk.tokenize import SyllableTokenizer\n",
        "        >>> from nltk import word_tokenize\n",
        "        >>> SSP = SyllableTokenizer()\n",
        "        >>> SSP.tokenize('justification')\n",
        "        ['jus', 'ti', 'fi', 'ca', 'tion']\n",
        "        >>> text = \"This is a foobar-like sentence.\"\n",
        "        >>> [SSP.tokenize(token) for token in word_tokenize(text)]\n",
        "        [['This'], ['is'], ['a'], ['foo', 'bar', '-', 'li', 'ke'], ['sen', 'ten', 'ce'], ['.']]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lang=\"en\", sonority_hierarchy=False):\n",
        "        \"\"\"\n",
        "        :param lang: Language parameter, default is English, 'en'\n",
        "        :type lang: str\n",
        "        :param sonority_hierarchy: Sonority hierarchy according to the\n",
        "                                   Sonority Sequencing Principle.\n",
        "        :type sonority_hierarchy: list(str)\n",
        "        \"\"\"\n",
        "        # Sonority hierarchy should be provided in descending order.\n",
        "        # If vowels are spread across multiple levels, they should be\n",
        "        # passed assigned self.vowels var together, otherwise should be\n",
        "        # placed in first index of hierarchy.\n",
        "        if not sonority_hierarchy and lang == \"en\":\n",
        "            sonority_hierarchy = [\n",
        "                \"aeiouy\",  # vowels.\n",
        "                \"lmnrw\",  # nasals.\n",
        "                \"zvsf\",  # fricatives.\n",
        "                \"bcdgtkpqxhj\",  # stops.\n",
        "            ]\n",
        "\n",
        "        self.vowels = sonority_hierarchy[0]\n",
        "        self.phoneme_map = {}\n",
        "        for i, level in enumerate(sonority_hierarchy):\n",
        "            for c in level:\n",
        "                sonority_level = len(sonority_hierarchy) - i\n",
        "                self.phoneme_map[c] = sonority_level\n",
        "                self.phoneme_map[c.upper()] = sonority_level\n",
        "\n",
        "    def assign_values(self, token):\n",
        "        \"\"\"\n",
        "        Assigns each phoneme its value from the sonority hierarchy.\n",
        "        Note: Sentence/text has to be tokenized first.\n",
        "\n",
        "        :param token: Single word or token\n",
        "        :type token: str\n",
        "        :return: List of tuples, first element is character/phoneme and\n",
        "                 second is the soronity value.\n",
        "        :rtype: list(tuple(str, int))\n",
        "        \"\"\"\n",
        "        syllables_values = []\n",
        "        for c in token:\n",
        "            try:\n",
        "                syllables_values.append((c, self.phoneme_map[c]))\n",
        "            except KeyError:\n",
        "                if c not in punctuation:\n",
        "                    warnings.warn(\n",
        "                        \"Character not defined in sonority_hierarchy,\"\n",
        "                        \" assigning as vowel: '{}'\".format(c)\n",
        "                    )\n",
        "                    syllables_values.append((c, max(self.phoneme_map.values())))\n",
        "                    self.vowels += c\n",
        "                else:  # If it's a punctuation, assing -1.\n",
        "                    syllables_values.append((c, -1))\n",
        "        return syllables_values\n",
        "\n",
        "\n",
        "    def validate_syllables(self, syllable_list):\n",
        "        \"\"\"\n",
        "        Ensures each syllable has at least one vowel.\n",
        "        If the following syllable doesn't have vowel, add it to the current one.\n",
        "\n",
        "        :param syllable_list: Single word or token broken up into syllables.\n",
        "        :type syllable_list: list(str)\n",
        "        :return: Single word or token broken up into syllables\n",
        "                 (with added syllables if necessary)\n",
        "        :rtype: list(str)\n",
        "        \"\"\"\n",
        "        valid_syllables = []\n",
        "        front = \"\"\n",
        "        for i, syllable in enumerate(syllable_list):\n",
        "            if syllable in punctuation:\n",
        "                valid_syllables.append(syllable)\n",
        "                continue\n",
        "            if not re.search(\"|\".join(self.vowels), syllable):\n",
        "                if len(valid_syllables) == 0:\n",
        "                    front += syllable\n",
        "                else:\n",
        "                    valid_syllables = valid_syllables[:-1] + [\n",
        "                        valid_syllables[-1] + syllable\n",
        "                    ]\n",
        "            else:\n",
        "                if len(valid_syllables) == 0:\n",
        "                    valid_syllables.append(front + syllable)\n",
        "                else:\n",
        "                    valid_syllables.append(syllable)\n",
        "\n",
        "        return valid_syllables\n",
        "\n",
        "\n",
        "    def tokenize(self, token):\n",
        "        \"\"\"\n",
        "        Apply the SSP to return a list of syllables.\n",
        "        Note: Sentence/text has to be tokenized first.\n",
        "\n",
        "        :param token: Single word or token\n",
        "        :type token: str\n",
        "        :return syllable_list: Single word or token broken up into syllables.\n",
        "        :rtype: list(str)\n",
        "        \"\"\"\n",
        "        # assign values from hierarchy\n",
        "        syllables_values = self.assign_values(token)\n",
        "\n",
        "        # if only one vowel return word\n",
        "        if sum(token.count(x) for x in self.vowels) <= 1:\n",
        "            return [token]\n",
        "\n",
        "        syllable_list = []\n",
        "        syllable = syllables_values[0][0]  # start syllable with first phoneme\n",
        "        for trigram in ngrams(syllables_values, n=3):\n",
        "            phonemes, values = zip(*trigram)\n",
        "            # Sonority of previous, focal and following phoneme\n",
        "            prev_value, focal_value, next_value = values\n",
        "            # Focal phoneme.\n",
        "            focal_phoneme = phonemes[1]\n",
        "\n",
        "            # These cases trigger syllable break.\n",
        "            if focal_value == -1:  # If it's a punctuation, just break.\n",
        "                syllable_list.append(syllable)\n",
        "                syllable_list.append(focal_phoneme)\n",
        "                syllable = \"\"\n",
        "            elif prev_value >= focal_value == next_value:\n",
        "                syllable += focal_phoneme\n",
        "                syllable_list.append(syllable)\n",
        "                syllable = \"\"\n",
        "\n",
        "            elif prev_value > focal_value < next_value:\n",
        "                syllable_list.append(syllable)\n",
        "                syllable = \"\"\n",
        "                syllable += focal_phoneme\n",
        "\n",
        "            # no syllable break\n",
        "            else:\n",
        "                syllable += focal_phoneme\n",
        "\n",
        "        syllable += syllables_values[-1][0]  # append last phoneme\n",
        "        syllable_list.append(syllable)\n",
        "\n",
        "        return self.validate_syllables(syllable_list)"
      ],
      "metadata": {
        "id": "VhYTIrDElyTC"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "d = cmudict.dict()\n",
        "syl = []\n",
        "\n",
        "def nsyl(word):\n",
        "    try:\n",
        "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
        "    except:\n",
        "        st = SyllableTokenizer()\n",
        "        return len(st.tokenize(word))\n",
        "\n",
        "def syl_count(i):\n",
        "  tokens = data['token_text'][i]\n",
        "  syl_tokens = [nsyl(t) for t in tokens]\n",
        "  syl_count_token = sum(syl_tokens)\n",
        "  return sum(syl_tokens)\n",
        "\n",
        "for i in tqdm(range(len(data['cleaned_body'])), desc='Progress'):\n",
        "    syl.append(syl_count(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvgcONZ3lzQi",
        "outputId": "50e106c0-2542-45e1-b9ac-b965f0425970"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress: 100%|██████████| 369494/369494 [12:02<00:00, 511.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['syllable_count'] = syl"
      ],
      "metadata": {
        "id": "4hEAI6pHnKOw"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff_words = []\n",
        "def tokens_no_nums(txt):\n",
        "  txt = re.sub('\\d', '', txt)\n",
        "  tokens = nltk.word_tokenize(txt)\n",
        "  words = [word for word in tokens if word.isalpha()]\n",
        "  return words\n",
        "\n",
        "def token_count(txt):\n",
        "  return len(tokens_no_nums(txt))\n",
        "\n",
        "def more_2_syl(txt):\n",
        "  count = 0\n",
        "  tokens = tokens_no_nums(txt)\n",
        "  syl_tokens = [nsyl(t) for t in tokens]\n",
        "  for s in syl_tokens:\n",
        "    if s > 2:\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def per_more_2_syl(txt):\n",
        "  if token_count(txt) != 0:\n",
        "    return 100*more_2_syl(txt)/token_count(txt)\n",
        "  else:\n",
        "    return \"\"\n",
        "\n",
        "for i in tqdm(range(0, len(data)), desc='Progress'):\n",
        "  \n",
        "  diff_words.append(per_more_2_syl(data['cleaned_body'][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxkc4D2euQha",
        "outputId": "9dc7397d-caf7-47be-e61b-1d7f0ee85866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress:   3%|▎         | 10278/369494 [00:21<20:50, 287.19it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_count(data['body'][187500]))"
      ],
      "metadata": {
        "id": "-felv98WPiFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['body'][353])"
      ],
      "metadata": {
        "id": "LxxcSehNyMnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['body'][187603])\n",
        "print(data['token_text'][187603])"
      ],
      "metadata": {
        "id": "SlX4Og3Jv0FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['percent_diff_words'] = diff_words"
      ],
      "metadata": {
        "id": "l_65VPLpvDkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['token_text'][0])\n",
        "y = data['token_text'][0]\n",
        "x = nltk.pos_tag(['gt', 'but', 'retirement', 'privilege', 'often'])\n",
        "print(x)"
      ],
      "metadata": {
        "id": "Jq4aUiD0-fUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "#data['token'] = data.apply(lambda row: nltk.word_tokenize(row['STORY']), axis=1)\n",
        "data['pos_tags'] = data.progress_apply(lambda row: nltk.pos_tag(nltk.word_tokenize(row['cleaned_body'])), axis=1)\n",
        "\n",
        "\n",
        "tag_count_data = pd.DataFrame(data['pos_tags'].map(lambda x: Counter(tag[1] for tag in x)).to_list())\n",
        "\n",
        "pd.concat([data, tag_count_data], axis=1).fillna(0).drop(['pos_tags'], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954,
          "referenced_widgets": [
            "3a28c9b4d9c644f69aab1821ed57b65a",
            "2c7eefe34c2748988b111919fd3aa921",
            "542ed864081c48059b7e0dee52740231",
            "3e301883799f49d481c53a4538851a19",
            "0bbc079d835a4e54a7484062fc33ad94",
            "ff139b3b02274c789281dc1687d1d086",
            "ccbccc5563f24f71abfdaf64756824b5",
            "5e1da346a23948228da1a649e5d4c8e9",
            "4c5d7e2487714bb8b456d0472dedf027",
            "3aa6ea9d6b06481e93588e30b0fdf600",
            "98d019214bff4b4e9d9c19447ab9e7b1"
          ]
        },
        "id": "ifyPHLOz-f2T",
        "outputId": "320fb53b-950c-4e46-9d50-b26575907ccc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/369494 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a28c9b4d9c644f69aab1821ed57b65a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       subreddit_id             author  \\\n",
              "0          t5_2qr1c  Agreeable_Task500   \n",
              "1          t5_2rfyw         thefumingo   \n",
              "2          t5_2r81q          joculator   \n",
              "3          t5_2rfyw       forfactorial   \n",
              "4          t5_2rfyw         sojuandbbq   \n",
              "...             ...                ...   \n",
              "369489     t5_2qr1c          ApacheNDN   \n",
              "369490     t5_2qr1c   EternityOnDemand   \n",
              "369491     t5_2r81q    Mace-Window_777   \n",
              "369492     t5_2rfz1    smell-the-roses   \n",
              "369493     t5_2r81q      GadgetGod1906   \n",
              "\n",
              "                                                     body  \\\n",
              "0       Yeah honestly, even as a black person from up ...   \n",
              "1       The opposite experience here; have rarely foun...   \n",
              "2       Memphis is the 26th most dangerous city in the...   \n",
              "3       compare the message from wandering earth 2 and...   \n",
              "4       If you’re serious or semi-serious you could lo...   \n",
              "...                                                   ...   \n",
              "369489                             Thanks. I'll tell her.   \n",
              "369490  I heard a black guy here in Canada telling me ...   \n",
              "369491  Did you report it to YouTube administration? T...   \n",
              "369492                                      Zebra people?   \n",
              "369493           I have to get the hell out of this state   \n",
              "\n",
              "                                             cleaned_body             race  \\\n",
              "0       Yeah honestly even as a black person from up n...  Native American   \n",
              "1       The opposite experience here have rarely found...            Asian   \n",
              "2       Memphis is the 26th most dangerous city in the...            Black   \n",
              "3       compare the message from wandering earth 2 and...            Asian   \n",
              "4       If youre serious or semiserious you could look...            Asian   \n",
              "...                                                   ...              ...   \n",
              "369489                                Thanks Ill tell her  Native American   \n",
              "369490  I heard a black guy here in Canada telling me ...  Native American   \n",
              "369491  Did you report it to YouTube administration Th...            Black   \n",
              "369492                                       Zebra people            White   \n",
              "369493           I have to get the hell out of this state            Black   \n",
              "\n",
              "        text_len_norm  text_length  \\\n",
              "0            9.547114         4552   \n",
              "1            0.243500          332   \n",
              "2           -0.205019          296   \n",
              "3           -0.522514           89   \n",
              "4            0.968534          562   \n",
              "...               ...          ...   \n",
              "369489      -0.417131           22   \n",
              "369490       0.016193          219   \n",
              "369491      -0.517575           78   \n",
              "369492      -0.545455           13   \n",
              "369493      -0.572057           40   \n",
              "\n",
              "                                               token_text  mean_w2v  \\\n",
              "0       ['yeah', 'honestly', 'even', 'black', 'person'...  0.057087   \n",
              "1       ['the', 'opposite', 'experience', 'rarely', 'f... -0.008365   \n",
              "2       ['memphis', '26th', 'dangerous', 'city', 'enti...  0.112700   \n",
              "3       ['compare', 'message', 'wandering', 'earth', '...  0.070321   \n",
              "4       ['if', 'youre', 'serious', 'semiserious', 'cou...  0.059412   \n",
              "...                                                   ...       ...   \n",
              "369489                          ['thanks', 'ill', 'tell']  0.021955   \n",
              "369490  ['i', 'heard', 'black', 'guy', 'canada', 'tell...  0.042735   \n",
              "369491  ['did', 'report', 'youtube', 'administration',... -0.097409   \n",
              "369492                                ['zebra', 'people'] -0.009624   \n",
              "369493                      ['i', 'get', 'hell', 'state']  0.089229   \n",
              "\n",
              "           ld_mtld  ...  WDT   FW  JJR  PDT  NNPS  RBR  RBS    $  WP$   LS  \n",
              "0        95.873689  ...  5.0  3.0  1.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "1       134.560000  ...  1.0  0.0  0.0  1.0   1.0  0.0  0.0  0.0  0.0  0.0  \n",
              "2        73.173333  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "3        11.000000  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "4       116.640000  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "...            ...  ...  ...  ...  ...  ...   ...  ...  ...  ...  ...  ...  \n",
              "369489    4.000000  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "369490  224.000000  ...  0.0  0.0  0.0  1.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "369491   13.000000  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "369492    2.000000  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "369493   10.000000  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "\n",
              "[369494 rows x 88 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cce6873-7a99-49f4-b886-ba7fcf080d38\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit_id</th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "      <th>cleaned_body</th>\n",
              "      <th>race</th>\n",
              "      <th>text_len_norm</th>\n",
              "      <th>text_length</th>\n",
              "      <th>token_text</th>\n",
              "      <th>mean_w2v</th>\n",
              "      <th>ld_mtld</th>\n",
              "      <th>...</th>\n",
              "      <th>WDT</th>\n",
              "      <th>FW</th>\n",
              "      <th>JJR</th>\n",
              "      <th>PDT</th>\n",
              "      <th>NNPS</th>\n",
              "      <th>RBR</th>\n",
              "      <th>RBS</th>\n",
              "      <th>$</th>\n",
              "      <th>WP$</th>\n",
              "      <th>LS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>t5_2qr1c</td>\n",
              "      <td>Agreeable_Task500</td>\n",
              "      <td>Yeah honestly, even as a black person from up ...</td>\n",
              "      <td>Yeah honestly even as a black person from up n...</td>\n",
              "      <td>Native American</td>\n",
              "      <td>9.547114</td>\n",
              "      <td>4552</td>\n",
              "      <td>['yeah', 'honestly', 'even', 'black', 'person'...</td>\n",
              "      <td>0.057087</td>\n",
              "      <td>95.873689</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>t5_2rfyw</td>\n",
              "      <td>thefumingo</td>\n",
              "      <td>The opposite experience here; have rarely foun...</td>\n",
              "      <td>The opposite experience here have rarely found...</td>\n",
              "      <td>Asian</td>\n",
              "      <td>0.243500</td>\n",
              "      <td>332</td>\n",
              "      <td>['the', 'opposite', 'experience', 'rarely', 'f...</td>\n",
              "      <td>-0.008365</td>\n",
              "      <td>134.560000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t5_2r81q</td>\n",
              "      <td>joculator</td>\n",
              "      <td>Memphis is the 26th most dangerous city in the...</td>\n",
              "      <td>Memphis is the 26th most dangerous city in the...</td>\n",
              "      <td>Black</td>\n",
              "      <td>-0.205019</td>\n",
              "      <td>296</td>\n",
              "      <td>['memphis', '26th', 'dangerous', 'city', 'enti...</td>\n",
              "      <td>0.112700</td>\n",
              "      <td>73.173333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>t5_2rfyw</td>\n",
              "      <td>forfactorial</td>\n",
              "      <td>compare the message from wandering earth 2 and...</td>\n",
              "      <td>compare the message from wandering earth 2 and...</td>\n",
              "      <td>Asian</td>\n",
              "      <td>-0.522514</td>\n",
              "      <td>89</td>\n",
              "      <td>['compare', 'message', 'wandering', 'earth', '...</td>\n",
              "      <td>0.070321</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>t5_2rfyw</td>\n",
              "      <td>sojuandbbq</td>\n",
              "      <td>If you’re serious or semi-serious you could lo...</td>\n",
              "      <td>If youre serious or semiserious you could look...</td>\n",
              "      <td>Asian</td>\n",
              "      <td>0.968534</td>\n",
              "      <td>562</td>\n",
              "      <td>['if', 'youre', 'serious', 'semiserious', 'cou...</td>\n",
              "      <td>0.059412</td>\n",
              "      <td>116.640000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369489</th>\n",
              "      <td>t5_2qr1c</td>\n",
              "      <td>ApacheNDN</td>\n",
              "      <td>Thanks. I'll tell her.</td>\n",
              "      <td>Thanks Ill tell her</td>\n",
              "      <td>Native American</td>\n",
              "      <td>-0.417131</td>\n",
              "      <td>22</td>\n",
              "      <td>['thanks', 'ill', 'tell']</td>\n",
              "      <td>0.021955</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369490</th>\n",
              "      <td>t5_2qr1c</td>\n",
              "      <td>EternityOnDemand</td>\n",
              "      <td>I heard a black guy here in Canada telling me ...</td>\n",
              "      <td>I heard a black guy here in Canada telling me ...</td>\n",
              "      <td>Native American</td>\n",
              "      <td>0.016193</td>\n",
              "      <td>219</td>\n",
              "      <td>['i', 'heard', 'black', 'guy', 'canada', 'tell...</td>\n",
              "      <td>0.042735</td>\n",
              "      <td>224.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369491</th>\n",
              "      <td>t5_2r81q</td>\n",
              "      <td>Mace-Window_777</td>\n",
              "      <td>Did you report it to YouTube administration? T...</td>\n",
              "      <td>Did you report it to YouTube administration Th...</td>\n",
              "      <td>Black</td>\n",
              "      <td>-0.517575</td>\n",
              "      <td>78</td>\n",
              "      <td>['did', 'report', 'youtube', 'administration',...</td>\n",
              "      <td>-0.097409</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369492</th>\n",
              "      <td>t5_2rfz1</td>\n",
              "      <td>smell-the-roses</td>\n",
              "      <td>Zebra people?</td>\n",
              "      <td>Zebra people</td>\n",
              "      <td>White</td>\n",
              "      <td>-0.545455</td>\n",
              "      <td>13</td>\n",
              "      <td>['zebra', 'people']</td>\n",
              "      <td>-0.009624</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369493</th>\n",
              "      <td>t5_2r81q</td>\n",
              "      <td>GadgetGod1906</td>\n",
              "      <td>I have to get the hell out of this state</td>\n",
              "      <td>I have to get the hell out of this state</td>\n",
              "      <td>Black</td>\n",
              "      <td>-0.572057</td>\n",
              "      <td>40</td>\n",
              "      <td>['i', 'get', 'hell', 'state']</td>\n",
              "      <td>0.089229</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>369494 rows × 88 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cce6873-7a99-49f4-b886-ba7fcf080d38')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0cce6873-7a99-49f4-b886-ba7fcf080d38 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0cce6873-7a99-49f4-b886-ba7fcf080d38');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['pos_tags'])"
      ],
      "metadata": {
        "id": "1kHCCKW-Y_DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_count_data"
      ],
      "metadata": {
        "id": "FQeVRad4ZbMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.concat([data, tag_count_data], axis=1).fillna(0).drop(['pos_tags'], axis=1)"
      ],
      "metadata": {
        "id": "9LZD0yiGZpLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching unique word count\n",
        "\n",
        "\n",
        "def ngram_unique(text):\n",
        "\n",
        "    unique = []\n",
        "    for word in text:\n",
        "        if word not in unique:\n",
        "            unique.append(word)\n",
        "    \n",
        "    return len(unique)/len(text)"
      ],
      "metadata": {
        "id": "wcoz49k5wXMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['unique_words'] = data['token_text'].progress_apply(ngram_unique)"
      ],
      "metadata": {
        "id": "tnzjJ08DwuM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['unique_words'])"
      ],
      "metadata": {
        "id": "ipzT-bHywPtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_data.columns)\n",
        "new_data.to_csv(\"/DATA/Processed/racial_groups.csv\", index=False)"
      ],
      "metadata": {
        "id": "wyrHq6TXZutU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}